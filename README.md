# ğŸš¢ Titanic Survival Prediction

[![Kaggle](https://img.shields.io/badge/Kaggle-20BEFF?style=for-the-badge&logo=Kaggle&logoColor=white)](https://www.kaggle.com/code/adinathjagtap777/titanic-machine-learning-from-disaster)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)

> **Top Score**: 0.80143 | Ensemble Decision Forest Model

---

## ğŸ“Š Competition Overview

This repository contains my solution for the [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition on Kaggle. The goal is to predict passenger survival based on various features like age, sex, ticket class, and more.

### ğŸ† Achievement
- **Public Score**: 0.80143
- **Approach**: Ensemble Gradient Boosted Trees

---

## ğŸ¯ Key Features

- **Clean, Production-Ready Code**: Minimal dependencies, maximum efficiency
- **Ensemble Learning**: 100 gradient boosted tree models for robust predictions
- **Feature Engineering**: Smart preprocessing of names, tickets, and categorical variables
- **Reproducible Results**: Seed-controlled training for consistency

---

## ğŸ› ï¸ Technical Stack

```
â”œâ”€â”€ TensorFlow Decision Forests (tfdf)
â”œâ”€â”€ Pandas & NumPy
â”œâ”€â”€ Gradient Boosted Trees
â””â”€â”€ Ensemble Averaging
```

---

## ğŸ“ Repository Structure

```
titanic-survival-prediction/
â”‚
â”œâ”€â”€ titanic_model.ipynb          # Main training notebook
â”œâ”€â”€ submission.csv               # Kaggle submission file (0.80143 score)
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone 

# Install dependencies
pip install -r requirements.txt
```

### Usage

```bash
# Run the notebook
jupyter notebook titanic_model.ipynb
```

Or use the provided cells directly in Kaggle notebooks.

---

## ğŸ§  Methodology

### 1. **Data Preprocessing**
- Name tokenization for title extraction
- Ticket splitting into prefix and number
- Handling missing values naturally through tree models

### 2. **Feature Engineering**
```python
Features Used:
â”œâ”€â”€ Pclass (Passenger Class)
â”œâ”€â”€ Name (Tokenized)
â”œâ”€â”€ Sex
â”œâ”€â”€ Age
â”œâ”€â”€ SibSp (Siblings/Spouses)
â”œâ”€â”€ Parch (Parents/Children)
â”œâ”€â”€ Fare
â”œâ”€â”€ Cabin
â”œâ”€â”€ Embarked
â”œâ”€â”€ Ticket_number (Engineered)
â””â”€â”€ Ticket_item (Engineered)
```

### 3. **Model Architecture**
- **Algorithm**: Gradient Boosted Trees
- **Ensemble Size**: 100 models
- **Technique**: Honest trees (separate data for structure and leaf values)
- **Averaging**: Probability averaging across all models

### 4. **Training Strategy**
```python
for i in range(100):
    model = GradientBoostedTreesModel(
        random_seed=i,
        honest=True
    )
    predictions += model.predict()
final_prediction = predictions / 100
```

---

## ğŸ“ˆ Results

| Metric | Score |
|--------|-------|
| **Public Leaderboard** | 0.80143 |
| **Best Score** | 0.80143 |
| **Models Used** | 100 |
| **Training Time** | ~3 minutes |

---

## ğŸ’¡ Key Insights

1. **Ensemble Power**: Averaging 100 models significantly improves stability
2. **Feature Engineering**: Ticket and name features provide valuable signals
3. **Honest Trees**: Reduces overfitting through data splitting
4. **Simplicity**: Clean code without complex hyperparameter tuning

---

## ğŸ”§ Requirements

```txt
tensorflow>=2.12.0
tensorflow-decision-forests>=1.2.0
pandas>=1.5.0
numpy>=1.23.0
```

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the [issues page](../../issues).

---

## ğŸ“§ Contact

**Adinath Jagtap**

- Kaggle: [@adinathjagtap777](https://www.kaggle.com/adinathjagtap777)
- GitHub: [@Adinath-Jagtap]([https://github.com/Adinath-Jagtap])

---

## â­ Show Your Support

Give a â­ï¸ if this project helped you!

---

## ğŸ“š References

- [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)
- [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests)
- [Gradient Boosted Trees](https://en.wikipedia.org/wiki/Gradient_boosting)

---

<div align="center">
  <sub>Built with â¤ï¸ for the Kaggle community</sub>
</div>
