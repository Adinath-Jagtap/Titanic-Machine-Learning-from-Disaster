<div align="center">
<h1>Titanic Survival Prediction</h1>
</div>
<div align="center">

[![Kaggle](https://img.shields.io/badge/Kaggle-20BEFF?style=for-the-badge&logo=Kaggle&logoColor=white)](https://www.kaggle.com/code/adinathjagtap777/titanic-machine-learning-from-disaster)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

**Machine Learning solution for Kaggle's Titanic competition using Ensemble Gradient Boosted Trees**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Methodology](#-methodology) â€¢ [Results](#-results)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Methodology](#-methodology)
- [Results](#-results)
- [Contact](#-contact)

---

## ğŸ¯ Overview

This repository contains a machine learning solution for the iconic [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition on Kaggle. The challenge involves predicting passenger survival based on features such as age, sex, ticket class, fare, and family relationships.

### Competition Objective

Build a predictive model that answers the question: *"What sorts of people were more likely to survive the Titanic disaster?"*

### Achievement

- **Public Leaderboard Score**: 0.80143
- **Approach**: Ensemble of 100 Gradient Boosted Tree models
- **Framework**: TensorFlow Decision Forests

---

## âœ¨ Features

- **Ensemble Learning Architecture** â€“ 100 gradient boosted tree models with probability averaging
- **Advanced Feature Engineering** â€“ Name title extraction, ticket parsing, and categorical encoding
- **Production-Ready Code** â€“ Clean, minimal dependencies with reproducible results
- **Efficient Training** â€“ Complete model training in approximately 3 minutes
- **Seed-Controlled Experiments** â€“ Deterministic results for research reproducibility

---

## ğŸ“ Project Structure

```
Titanic-Machine-Learning-from-Disaster/
â”‚
â”œâ”€â”€ 100% Score CSV File/
â”‚   â””â”€â”€ Submission.csv                    # Reference submission file
â”‚
â”œâ”€â”€ Titanic - Machine Learning from Disaster/
â”‚   â”œâ”€â”€ requirement.txt                   # Python dependencies
â”‚   â”œâ”€â”€ submission.csv                    # Generated predictions (0.80143)
â”‚   â”œâ”€â”€ titanic-machine-learning-from-disaster.ipynb
â”‚   â””â”€â”€ README.md                         # Project documentation
â”‚
â””â”€â”€ README.md                             # This file
```

> **Note**: The `100% Score CSV File` directory contains a reference submission for benchmarking purposes.

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Jupyter Notebook (optional, for running `.ipynb` files)

### Setup

1. **Clone the repository**

```bash
git clone https://github.com/Adinath-Jagtap/Titanic-Machine-Learning-from-Disaster.git
cd Titanic-Machine-Learning-from-Disaster
```

2. **Navigate to project directory**

```bash
cd "Titanic - Machine Learning from Disaster"
```

3. **Install dependencies**

```bash
pip install -r requirement.txt
```

### Required Libraries

```
tensorflow>=2.12.0
tensorflow-decision-forests>=1.2.0
pandas>=1.5.0
numpy>=1.23.0
```

---

## ğŸ’» Usage

### Running the Notebook

**Option 1: Local Jupyter**

```bash
jupyter notebook titanic-machine-learning-from-disaster.ipynb
```

**Option 2: Kaggle Kernels**

Upload the notebook directly to [Kaggle](https://www.kaggle.com/code/adinathjagtap777/titanic-machine-learning-from-disaster) and run with GPU acceleration enabled.

### Training the Model

The notebook follows this workflow:

1. Load training and test datasets
2. Preprocess features (name tokenization, ticket parsing)
3. Train ensemble of 100 Gradient Boosted Tree models
4. Generate predictions through probability averaging
5. Export `submission.csv` for Kaggle submission

---

## ğŸ§  Methodology

### Data Preprocessing

The preprocessing pipeline handles missing values and engineers new features:

| Feature | Processing | Purpose |
|---------|-----------|---------|
| **Name** | Tokenization & title extraction | Extract social status indicators (Mr., Mrs., etc.) |
| **Ticket** | Split into prefix + number | Identify ticket groups and classes |
| **Cabin** | Keep as-is | Tree models handle missing values naturally |
| **Age** | No imputation | Decision trees manage gaps automatically |
| **Fare** | Numeric | Direct usage |

### Feature Set

```
Input Features:
â”œâ”€â”€ Pclass          â†’ Passenger class (1st, 2nd, 3rd)
â”œâ”€â”€ Name            â†’ Tokenized for title extraction
â”œâ”€â”€ Sex             â†’ Male/Female
â”œâ”€â”€ Age             â†’ Passenger age
â”œâ”€â”€ SibSp           â†’ Number of siblings/spouses aboard
â”œâ”€â”€ Parch           â†’ Number of parents/children aboard
â”œâ”€â”€ Fare            â†’ Ticket price
â”œâ”€â”€ Cabin           â†’ Cabin identifier (if available)
â”œâ”€â”€ Embarked        â†’ Port of embarkation (C, Q, S)
â”œâ”€â”€ Ticket_number   â†’ Engineered from ticket
â””â”€â”€ Ticket_item     â†’ Engineered ticket prefix
```

### Model Architecture

**Algorithm**: Gradient Boosted Trees with Honest Learning

```python
Model Configuration:
â”œâ”€â”€ Number of Models: 100
â”œâ”€â”€ Random Seeds: 0-99 (for diversity)
â”œâ”€â”€ Honest Trees: True (separate structure/leaf data)
â”œâ”€â”€ Ensemble Method: Probability averaging
â””â”€â”€ Training Strategy: Sequential with different seeds
```

**Key Algorithm Properties**:
- **Honest Trees** reduce overfitting by splitting data for structure learning and leaf value estimation
- **Ensemble Diversity** achieved through varied random seeds
- **Probability Averaging** produces more stable predictions than voting

---

## ğŸ“Š Results

### Performance Metrics

| Metric | Value |
|--------|-------|
| **Public Leaderboard Score** | 0.80143 |
| **Number of Models** | 100 |
| **Training Duration** | ~3 minutes |
| **Feature Count** | 11 |

### Key Insights

1. **Ensemble Strength**: Averaging 100 models significantly reduces variance and improves generalization
2. **Feature Engineering Impact**: Ticket parsing and name tokenization capture hidden survival patterns
3. **Honest Trees Benefit**: Data splitting for structure/values prevents overfitting better than standard boosting
4. **Simplicity Wins**: Clean preprocessing without excessive hyperparameter tuning achieves strong results

---

## ğŸ”¬ Technical Details

### Why Gradient Boosted Trees?

- **Handles mixed data types** naturally (numeric, categorical, text)
- **Robust to missing values** without imputation
- **Captures non-linear relationships** automatically
- **Fast training** compared to deep learning approaches
- **Interpretable** through feature importance analysis

### Why Ensemble of 100 Models?

- **Reduces overfitting** through model diversity
- **Stabilizes predictions** via averaging
- **Captures different aspects** of data through varied random seeds
- **Low computational cost** due to efficient tree training

---

## ğŸ“§ Contact

**Adinath Jagtap**

- **Kaggle**: [@adinathjagtap777](https://www.kaggle.com/adinathjagtap777)
- **GitHub**: [@Adinath-Jagtap](https://github.com/Adinath-Jagtap)

Feel free to reach out for collaboration or questions!

---

## ğŸ“š References

- [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic) â€“ Official competition page
- [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) â€“ Framework documentation
- [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) â€“ Algorithm overview
- [Honest Trees Paper](https://arxiv.org/abs/1504.01132) â€“ Research on honest splitting

---

## â­ Acknowledgments

Special thanks to the Kaggle community for inspiration and the TensorFlow team for Decision Forests library.

---

<div align="center">

**If this project helped you, please consider giving it a â­**

Made with dedication for the Kaggle community

</div>
